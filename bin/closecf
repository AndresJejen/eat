#!/usr/bin/env python

# close ALIST and create HOPS control file steered to global solution
# 2016-10-11 Lindy Blackburn
# 2017-07-20 Lindy Blackburn & CK Chan - update for 2015+ data

from eat.io import hops, util
import numpy as np
from scipy.optimize import least_squares
from itertools import chain
import pandas as pd
import argparse
import os

parser = argparse.ArgumentParser()
parser.add_argument('filename', help='alist txt file')
parser.add_argument('-de', '--delay_error', help='delay error systematic in [us]', type=float, default=3e-6)
parser.add_argument('-re', '--rate_error', help='rate error systematic in [ps/s]', type=float, default=1e-3)
parser.add_argument('-ce', '--crosspol_delay_error', help='additional systematic for cross pol delay added in quadrature', type=float, default=20e-6)
parser.add_argument('-pp', '--polarization_products', help='separate delays by polarization product', action='store_true')
parser.add_argument('-sw', '--sigmas_to_window', help='sigmas to window for both delay and rate', type=float, default=0)
parser.add_argument('-dw', '--delay_window', help='fixed delay window to apply in [us]', type=float, default=0)
parser.add_argument('-rw', '--rate_window', help='fixed rate window to apply in [ps/s]', type=float, default=0)
args = parser.parse_args()

# read alist file (autodetect version 5 or 6)
a = hops.read_alist(args.filename)

# stations grouped according to having related fringe parameters
if int(a.iloc[0].year) <= 2013:
    dishes = ['DE', 'FG', 'J', 'PQ', 'ST', 'A']
    feeds = [l for l in "DEFGJPQSTA"]
elif args.polarization_products:
    # ['J', 'L', 'S', 'Z']
    dishes = sorted(set(chain(*a.baseline)))
    # [('L', 'L'), ('L', 'R'), ('S', 'L'), ('S', 'R'), ('Z', 'L'), ('Z', 'R')]
    feeds = sorted(set(chain(*[zip(*q) for q in zip(a.baseline, a.polarization)])))
else:
    dishes = sorted(set(chain(*a.baseline)))
    feeds = dishes

# reverse index of lookup for single dish station code
idish = {f:i for i, flist in enumerate(dishes) for f in flist}
ifeed = {f:i for i, f in enumerate(feeds)}

# unwrap mbd to be aligned to sbd
util.unwrap_mbd(a)

# add delay and rate errors, with systematic limits on resolution
util.add_delayerr(a, mbd_systematic=args.delay_error, rate_systematic=args.rate_error,
                     crosspol_systematic=args.crosspol_delay_error)

# least squares objective function: (predict - model) / std
# par is a list of the mdoel parameters fit against alldata and allerr
# idx1: the iloc of the first HOPS station in each baseline
# idx2: the iloc of the second HOPS station in each baseline
def errfunc(par, idx1, idx2, alldata, allerr):
    model = par[idx1] - par[idx2]
    return (alldata - model) / allerr

# indices for unique feeds/dishes
a['idish0'] = [idish[bl[0]] for bl in a.baseline]
a['idish1'] = [idish[bl[1]] for bl in a.baseline]
if int(a.iloc[0].year) <= 2013:
    a['ifeed0'] = [ifeed[bl[0]] for bl in a.baseline]
    a['ifeed1'] = [ifeed[bl[1]] for bl in a.baseline]
elif args.polarization_products:
    a['ifeed0'] = [ifeed[(bl[0], pol[0])] for (bl, pol) in zip(a.baseline, a.polarization)]
    a['ifeed1'] = [ifeed[(bl[1], pol[1])] for (bl, pol) in zip(a.baseline, a.polarization)]
else:
    a['ifeed0'] = [ifeed[bl[0]] for bl in a.baseline]
    a['ifeed1'] = [ifeed[bl[1]] for bl in a.baseline]

g = a.groupby('scan_id')
scans = sorted(set(a.scan_id))

# loop over all scans and overwrite with new solution
# for scan in scans:
for scan in scans:
    idx = g.groups[scan]
    # skip if only one baseline (avoid warning)
    if len(idx) < 2:
        continue
    b = a.loc[idx]
    # initial guess
    rates = np.zeros(len(dishes))
    delays = np.zeros(len(feeds))
    # f_scale will be the deviation [in sigma] where the loss function kicks in
    # it should be a function of the SNR of the detection ideally..
    # but it looks like scipy just supports a single float
    fit_mbd = least_squares(errfunc, np.zeros(len(feeds)),
                            args=(b.ifeed0, b.ifeed1, b.mbd_unwrap, b.mbd_err),
                            loss='arctan', f_scale=6).x
    fit_rate = least_squares(errfunc, np.zeros(len(dishes)),
                            args=(b.idish0, b.idish1, b.delay_rate, b.rate_err),
                            loss='arctan', f_scale=6).x
    a.ix[idx,'mbd_unwrap'] = fit_mbd[b.ifeed0] - fit_mbd[b.ifeed1]
    a.ix[idx,'delay_rate'] = fit_rate[b.idish0] - fit_rate[b.idish1]

util.rewrap_mbd(a)

# print out FF control file

if args.polarization_products:
    for pp in set(a.polarization):
        out = open('%s.closecf.%s' % (os.path.split(args.filename)[-1], pp), 'w')
        app = a[a.polarization == pp]
        g = app.groupby('timetag')
        for (scan, b) in g:
            for c in b.itertuples():
                out.write("if scan %s and baseline %s sb_win %9.6f %9.6f mb_win %9.6f %9.6f dr_win %15.12f %15.12f\n" %
                    (c.timetag, c.baseline, c.mbd_unwrap, c.mbd_unwrap, c.mbdelay, c.mbdelay, 1e-6*c.delay_rate, 1e-6*c.delay_rate))
        out.close()
else:
    g = a.groupby('timetag')
    for (scan, b) in g:
        for c in b.itertuples():
            print("if scan %s and baseline %s sb_win %9.6f %9.6f mb_win %9.6f %9.6f dr_win %15.12f %15.12f" %
                (c.timetag, c.baseline, c.mbd_unwrap, c.mbd_unwrap, c.mbdelay, c.mbdelay, 1e-6*c.delay_rate, 1e-6*c.delay_rate))

